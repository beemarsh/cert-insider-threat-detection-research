{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76aeee9-b195-4c81-9978-e77c353d078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input dataset should already be the email dataset. \n",
    "# We only need three columns \"id\", \"user\", and \"content\" to analyse email content\n",
    "# We are also using the chunks that we generated using the email_chunk_generator file\n",
    "\n",
    "# The output dataset has six columns, 2 of which are user and day.\n",
    "# This script first drops all columns except the three. In this script, we clean the content of the email (i.e removing irrelevant characters, punctuation, and stop words (common words that don't add much meaning to the text) )\n",
    "# Then we divide the preprocessed text into individual tokens, which are meaningful units of text, typically words or phrases. This step breaks down the text into a format suitable for LDA processing\n",
    "# Then we construct a document-term matrix (DTM), where each row represents an email and each column represents a term from the vocabulary. The cells of the matrix contain the frequency of each term in each email.\n",
    "# We then create a vocabulary of unique tokens extracted from the entire email dataset. This vocabulary will be used to represent the words in each email.\n",
    "# Next, we apply LDA to the DTM to identify hidden topics within the email corpus. LDA assumes that each email is a mixture of these topics, and the model learns the probability distribution of topics for each email.\n",
    "# For each email, we extract the probability distribution of topics, representing the likelihood of each topic's presence in that email. This topic distribution vector will serve as a numerical representation of the email's content.\n",
    "\n",
    "\n",
    "# For this dataset calculation, I used HPC Cluster (Magnolia) from University of Southern Mississippi\n",
    "# In HPC clusters, I used Slrum Workload Manager, the script for which is also discussed somewhere in the repo\n",
    "\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f66d031f-7de2-40c9-a99e-c2a94a2f8023",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_folder_results = 'temp_results'  # Temporary folder to store intermediate result files\n",
    "temp_folder_chunks= 'temp_chunks'  # Folder that stores intermediate chunked files\n",
    "output_file = 'with_content_probability_distribution.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "449db9cd-c0c3-473b-a2d8-6b2170c2c930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1006)>\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1006)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98b6d2d-9d5c-4aa5-a441-1d4b3a3a15f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
