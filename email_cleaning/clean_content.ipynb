{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e76aeee9-b195-4c81-9978-e77c353d078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input dataset should be the chunks generated by content_chunks_generator. \n",
    "\n",
    "\n",
    "# This script first drops all columns except the three. In this script, we clean the content of the email (i.e removing irrelevant characters, punctuation, and stop words (common words that don't add much meaning to the text) )\n",
    "# Then we divide the preprocessed text into individual tokens, which are meaningful units of text, typically words or phrases. This step breaks down the text into a format suitable for LDA processing\n",
    "# Then we construct a document-term matrix (DTM), where each row represents an email and each column represents a term from the vocabulary. The cells of the matrix contain the frequency of each term in each email.\n",
    "# We then create a vocabulary of unique tokens extracted from the entire email dataset. This vocabulary will be used to represent the words in each email.\n",
    "# Next, we apply LDA to the DTM to identify hidden topics within the email corpus. LDA assumes that each email is a mixture of these topics, and the model learns the probability distribution of topics for each email.\n",
    "# For each email, we extract the probability distribution of topics, representing the likelihood of each topic's presence in that email. This topic distribution vector will serve as a numerical representation of the email's content.\n",
    "\n",
    "\n",
    "# For this dataset calculation, I used HPC Cluster (Magnolia) from University of Southern Mississippi\n",
    "# In HPC clusters, I used Slrum Workload Manager, the script for which is also discussed somewhere in the repo\n",
    "\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import ssl\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import argparse\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8657fa19-1586-486e-b259-cf152123f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f66d031f-7de2-40c9-a99e-c2a94a2f8023",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_folder_results = 'temp_results'  # Temporary folder to store intermediate result files\n",
    "temp_folder_chunks= 'temp_content_chunks'  # Folder that stores intermediate chunked files\n",
    "output_file = 'with_content_cleaned.csv'\n",
    "img_folder = 'images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00d416d0-2450-4280-984b-88d87dafe329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure that the folder exists\n",
    "os.makedirs(temp_folder_results, exist_ok=True)\n",
    "os.makedirs(img_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "449db9cd-c0c3-473b-a2d8-6b2170c2c930",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a98b6d2d-9d5c-4aa5-a441-1d4b3a3a15f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Apply stemming (using Porter Stemmer)\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_text = [stemmer.stem(word) for word in filtered_text]\n",
    "    \n",
    "    return ' '.join(stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51931b15-e589-4756-9021-d513eabc499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk_filename):\n",
    "    chunk = pd.read_csv(f'{temp_folder_chunks}/{chunk_filename}')\n",
    "    chunk['clean_content'] = chunk['content'].apply(clean_text)\n",
    "\n",
    "    chunk = chunk.drop('content',axis=1)\n",
    "    \n",
    "\n",
    "    temp_filename = f'{temp_folder_results}/temp_result_{chunk_filename}'\n",
    "    chunk.to_csv(temp_filename,index=False)\n",
    "\n",
    "    return temp_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d678e887-b891-49f9-8a67-58ae26706546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we have to get the list of chunks that we have in the chunks folder\n",
    "file_names = os.listdir(temp_folder_chunks)\n",
    "# Filter only files (not directories)\n",
    "\n",
    "if test:\n",
    "    chunk_filenames = ['temp_content_chunk_2010-01-03.csv','temp_content_chunk_2010-01-04.csv']\n",
    "else:\n",
    "    chunk_filenames = [file for file in file_names if os.path.isfile(os.path.join(temp_folder_chunks, file))]\n",
    "\n",
    "# Clean the texts parallelly\n",
    "with Pool() as pool:\n",
    "    result_filenames = pool.map(process_chunk, chunk_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c1926b2-66d3-4ba7-b9ac-2540fc377c5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_filenames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Since our results are divided into different files for each day, we have to combine them\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m combined_result \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([pd\u001b[38;5;241m.\u001b[39mread_csv(filename) \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresult_filenames\u001b[49m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Save the final result to a CSV file\u001b[39;00m\n\u001b[1;32m      5\u001b[0m combined_result\u001b[38;5;241m.\u001b[39mto_csv(output_file, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result_filenames' is not defined"
     ]
    }
   ],
   "source": [
    "# Since our results are divided into different files for each day, we have to combine them\n",
    "combined_result = pd.concat([pd.read_csv(filename) for filename in result_filenames])\n",
    "\n",
    "# Save the final result to a CSV file\n",
    "combined_result.to_csv(output_file, index=False)\n",
    "\n",
    "combined_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bde0816-4e88-46a5-82e0-77752c5fde33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete the temp files\n",
    "# # To remove the generated temp files\n",
    "# for filename in result_filenames:\n",
    "#     if os.path.exists(filename):  # Check if the file exists before removing\n",
    "#         os.remove(filename)\n",
    "#     else:\n",
    "#         print(f\"File {filename} not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
