{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e76aeee9-b195-4c81-9978-e77c353d078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input dataset should already be the email dataset. \n",
    "# We only need three columns \"id\", \"user\", and \"content\" to analyse email content\n",
    "# We are also using the chunks that we generated using the email_chunk_generator file\n",
    "\n",
    "# The output dataset has six columns, 2 of which are user and day.\n",
    "# This script first drops all columns except the three. In this script, we clean the content of the email (i.e removing irrelevant characters, punctuation, and stop words (common words that don't add much meaning to the text) )\n",
    "# Then we divide the preprocessed text into individual tokens, which are meaningful units of text, typically words or phrases. This step breaks down the text into a format suitable for LDA processing\n",
    "# Then we construct a document-term matrix (DTM), where each row represents an email and each column represents a term from the vocabulary. The cells of the matrix contain the frequency of each term in each email.\n",
    "# We then create a vocabulary of unique tokens extracted from the entire email dataset. This vocabulary will be used to represent the words in each email.\n",
    "# Next, we apply LDA to the DTM to identify hidden topics within the email corpus. LDA assumes that each email is a mixture of these topics, and the model learns the probability distribution of topics for each email.\n",
    "# For each email, we extract the probability distribution of topics, representing the likelihood of each topic's presence in that email. This topic distribution vector will serve as a numerical representation of the email's content.\n",
    "\n",
    "\n",
    "# For this dataset calculation, I used HPC Cluster (Magnolia) from University of Southern Mississippi\n",
    "# In HPC clusters, I used Slrum Workload Manager, the script for which is also discussed somewhere in the repo\n",
    "\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "\n",
    "import ssl\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "# nltk.data.path.append(\"./packages\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f66d031f-7de2-40c9-a99e-c2a94a2f8023",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_folder_results = 'temp_results'  # Temporary folder to store intermediate result files\n",
    "temp_folder_chunks= 'temp_chunks'  # Folder that stores intermediate chunked files\n",
    "output_file = 'with_content_probability_distribution.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "449db9cd-c0c3-473b-a2d8-6b2170c2c930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /homes/01/bxbhusal/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /homes/01/bxbhusal/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a98b6d2d-9d5c-4aa5-a441-1d4b3a3a15f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Apply stemming (using Porter Stemmer)\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_text = [stemmer.stem(word) for word in filtered_text]\n",
    "    \n",
    "    return ' '.join(stemmed_text)\n",
    "\n",
    "def process_chunk(chunk_filename):\n",
    "    chunk = pd.read_csv(f'{temp_folder_chunks}/{chunk_filename}')\n",
    "    \n",
    "    # Clean text\n",
    "    \n",
    "    chunk['clean_content'] = chunk['content'].apply(clean_text)\n",
    "    \n",
    "    # Create document-term matrix\n",
    "    # dtm_chunk = create_dtm_chunk(cleaned_chunk)\n",
    "    return chunk\n",
    "\n",
    "    # return dtm_chunk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d678e887-b891-49f9-8a67-58ae26706546",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/modules/python/3.11.5/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3790, in get_loc\n    return self._engine.get_loc(casted_key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"index.pyx\", line 152, in pandas._libs.index.IndexEngine.get_loc\n  File \"index.pyx\", line 181, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'content'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/modules/python/3.11.5/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/modules/python/3.11.5/lib/python3.11/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n           ^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_193909/3035168386.py\", line 27, in process_chunk\n    chunk['clean_content'] = chunk['content'].apply(clean_text)\n                             ~~~~~^^^^^^^^^^^\n  File \"/modules/python/3.11.5/lib/python3.11/site-packages/pandas/core/frame.py\", line 3896, in __getitem__\n    indexer = self.columns.get_loc(key)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/modules/python/3.11.5/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3797, in get_loc\n    raise KeyError(key) from err\nKeyError: 'content'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Calculate the number of emails sent and received during day and night for each chunk\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool() \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 10\u001b[0m     result_filenames \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_filenames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Since our results are divided into different files for each day, we have to combine them\u001b[39;00m\n\u001b[1;32m     13\u001b[0m combined_result \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([pd\u001b[38;5;241m.\u001b[39mread_csv(filename) \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m result_filenames])\n",
      "File \u001b[0;32m/modules/python/3.11.5/lib/python3.11/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/modules/python/3.11.5/lib/python3.11/multiprocessing/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mKeyError\u001b[0m: 'content'"
     ]
    }
   ],
   "source": [
    "# First we have to get the list of chunks that we have in the chunks folder\n",
    "file_names = os.listdir(temp_folder_chunks)\n",
    "# Filter only files (not directories)\n",
    "# chunk_filenames = [file for file in file_names if os.path.isfile(os.path.join(temp_folder_chunks, file))]\n",
    "\n",
    "chunk_filenames = ['temp_chunk_2010-01-04.csv','temp_chunk_2010-01-05.csv']\n",
    "\n",
    "# Calculate the number of emails sent and received during day and night for each chunk\n",
    "with Pool() as pool:\n",
    "    result_filenames = pool.map(process_chunk, chunk_filenames)\n",
    "\n",
    "# Since our results are divided into different files for each day, we have to combine them\n",
    "combined_result = pd.concat([pd.read_csv(filename) for filename in result_filenames])\n",
    "\n",
    "# Save the final result to a CSV file\n",
    "combined_result.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d4246a-4556-4117-9e4a-76e022b1182a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
