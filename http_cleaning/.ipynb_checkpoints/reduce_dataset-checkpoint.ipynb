{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5551521d-5060-4bed-a33b-1735637fe94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input dataset is the original Cert Insider Threat Dataset\n",
    "# We are using the latest r6.2 dataset\n",
    "\n",
    "# This script drops the content column which saves a lot a memory during calculating attributes\n",
    "# We will use http content later to develop a different model\n",
    "# Also, we added a 'day' column which allows us to group the data by day as we are calculating most of the attributes for each single day\n",
    "# We are also adding is_working_hour column\n",
    "\n",
    "# For this dataset calculation, I used HPC Cluster (Magnolia) from University of Southern Mississippi\n",
    "# In HPC clusters, I used Slrum Workload Manager, the script for which is also discussed somewhere in the repo\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b8f5e6-502c-4708-b652-70d7b71ce7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../../r6.2/http.csv'  # Path to your Email dataset\n",
    "output_file = 'reduced_http_dataset.csv'  # Output file\n",
    "temp_folder_combined = 'temp_combined'  # Temporary folder to store intermediate result files\n",
    "chunk_size = 500000  # Define an appropriate chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075eee2-7b6c-4d15-b9b5-378fa81fe8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure that the folder exists\n",
    "os.makedirs(temp_folder_combined, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe598d-8575-4d07-8f94-5043d9b011c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_hours_start = pd.to_datetime('09:00').time()\n",
    "working_hours_end = pd.to_datetime('17:00').time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79588cd-2c77-4ea1-ac70-febe76dcd360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_working_hours(timestamp):\n",
    "    time = timestamp.time()\n",
    "    # Check if it's a weekday (Monday: 0, Tuesday: 1, ..., Friday: 4)\n",
    "    if timestamp.weekday() in range(0, 5):\n",
    "        # Check if it's working hours or not\n",
    "        if working_hours_start <= time <= working_hours_end:\n",
    "            return True\n",
    "    return False  # It's not a weekday or not within working hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc8c1ba-ff59-479f-a3fe-f21ef12cf243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to combine 'to', 'cc', and 'bcc' columns into 'recipients' column\n",
    "def combine_recipients(args):\n",
    "    chunk, index = args\n",
    "\n",
    "    # Drop the 'content' column if it exists\n",
    "    if 'content' in chunk.columns:\n",
    "        chunk = chunk.drop(columns=['content'])\n",
    "\n",
    "    chunk['date'] = pd.to_datetime(chunk['date'], format='mixed')\n",
    "    chunk['is_working_hour'] = chunk['date'].apply(is_working_hours)\n",
    "    chunk['day'] = pd.to_datetime(chunk['date']).dt.date  # Extracting the date without time\n",
    "    chunk.to_csv(f'{temp_folder_combined}/temp_combined_{index}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd7d43-6864-4cab-bd59-8f2c556a8954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each chunk and save it into a temporary file in parallel\n",
    "with Pool() as pool:\n",
    "    chunks = [(chunk, i) for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size))]\n",
    "    pool.map(combine_recipients, chunks)\n",
    "\n",
    "# Merge all temporary files into one sorted file\n",
    "all_temp_files = [f'{temp_folder_combined}/temp_combined_{i}.csv' for i in range(len(chunks))]\n",
    "combined_data = pd.concat([pd.read_csv(f) for f in all_temp_files], ignore_index=True)\n",
    "combined_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6291b34-f23a-4d5f-9eb4-267624507389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary files\n",
    "for f in all_temp_files:\n",
    "    os.remove(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
