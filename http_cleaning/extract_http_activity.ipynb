{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0ab34d-1e1e-49d4-b1d8-948b947cd127",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The input dataset should already have a 'day' and 'is_working_hour column' defined which we already did.\n",
    "# We are also using the chunks that we generated using the http_chunk_generator file\n",
    "\n",
    "# The output dataset has eight columns, 2 of which are user and day.\n",
    "# This script calculate four attributes from the email cert dataset. It generates the following attribute for each user for every single day\n",
    "# numWebAccDay\n",
    "# numWebAccNight\n",
    "# numUploadDay\n",
    "# numUploadNight\n",
    "# numDownloadDay\n",
    "# numDownloadNight\n",
    "\n",
    "# For this dataset calculation, I used HPC Cluster (Magnolia) from University of Southern Mississippi\n",
    "# In HPC clusters, I used Slrum Workload Manager, the script for which is also discussed somewhere in the repo\n",
    "\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02348646-1ae6-42cf-96fa-37fad054370e",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_folder_results = 'temp_results'  # Temporary folder to store intermediate result files\n",
    "temp_folder_chunks= 'temp_chunks'  # Folder that stores intermediate chunked files\n",
    "output_file = 'with_activity_counts.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977a3caf-6538-47fd-b495-fbab60ac01e2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To make sure that the folder exists\n",
    "os.makedirs(temp_folder_results, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8e9eda-a441-4799-a4db-f8e476e5712d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate the number of emails sent and received during day and night for each user in a day\n",
    "def calculate_activity_counts(chunk_filename):\n",
    "    df = pd.read_csv(f'{temp_folder_chunks}/{chunk_filename}')\n",
    "\n",
    "    df['is_working_hour'] = df['is_working_hour'].astype(bool)\n",
    "\n",
    "    # Filter data for Sent activities\n",
    "    access_data = df[df['activity'] == 'WWW Visit']\n",
    "    download_data = df[df['activity'] == 'WWW Download']\n",
    "    upload_data = df[df['activity'] == 'WWW Upload']\n",
    "\n",
    "    # Group data by user, date, and working hour, then count the downloads during day and night for each user\n",
    "    day_access = access_data[access_data['is_working_hour']][['user', 'day']].groupby(['user', 'day']).size().reset_index(name='numWebAccDay')\n",
    "    night_access = access_data[~access_data['is_working_hour']][['user', 'day']].groupby(['user', 'day']).size().reset_index(name='numWebAccNight')\n",
    "    \n",
    "    # Group data by user, date, and working hour, then count the downloads during day and night for each user\n",
    "    day_downloads = download_data[download_data['is_working_hour']][['user', 'day']].groupby(['user', 'day']).size().reset_index(name='numDownloadsDay')\n",
    "    night_downloads = download_data[~download_data['is_working_hour']][['user', 'day']].groupby(['user', 'day']).size().reset_index(name='numDownloadsNight')\n",
    "\n",
    "    day_uploads = upload_data[upload_data['is_working_hour']][['user', 'day']].groupby(['user', 'day']).size().reset_index(name='numUploadDay')\n",
    "    night_uploads = upload_data[~upload_data['is_working_hour']][['user', 'day']].groupby(['user', 'day']).size().reset_index(name='numUploadNight')\n",
    "\n",
    "    combined_access = day_access.merge(night_access, on=['user', 'day'], how='outer')\n",
    "    combined_downloads = day_downloads.merge(night_downloads, on=['user', 'day'], how='outer')\n",
    "    combined_uploads = day_uploads.merge(night_uploads, on=['user', 'day'], how='outer')\n",
    "\n",
    "    merged_data = combined_access.merge(combined_downloads, on=['user', 'day'], how='outer')\n",
    "    merged_data = merged_data.merge(combined_uploads, on=['user', 'day'], how='outer')\n",
    "\n",
    "    merged_data.fillna(0, inplace=True)  # Replace NaN with 0\n",
    "    merged_data[['numWebAccDay','numWebAccNight','numDownloadsDay','numDownloadsNight','numUploadDay','numUploadNight']] = merged_data[['numWebAccDay','numWebAccNight','numDownloadsDay','numDownloadsNight','numUploadDay','numUploadNight']].astype(int)\n",
    "    \n",
    "    temp_filename = f'{temp_folder_results}/temp_result_{chunk_filename}'\n",
    "    \n",
    "    # # Because we are processing different chunks, a separate result file is generated for each chunk in temp_folder_chunks folder\n",
    "    merged_data.to_csv(temp_filename, index=False)\n",
    "    \n",
    "    return temp_filename  # Return the filename of the saved result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9af93-6c60-4956-9f09-c3eb825d9028",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First we have to get the list of chunks that we have in the chunks folder\n",
    "file_names = os.listdir(temp_folder_chunks)\n",
    "# Filter only files (not directories)\n",
    "chunk_filenames = [file for file in file_names if os.path.isfile(os.path.join(temp_folder_chunks, file))]\n",
    "\n",
    "# chunk_filenames = ['temp_chunk_2010-01-09.csv','temp_chunk_2010-01-12.csv']\n",
    "\n",
    "# Calculate the number of emails sent and received during day and night for each chunk\n",
    "with Pool() as pool:\n",
    "    result_filenames = pool.map(calculate_activity_counts, chunk_filenames)\n",
    "\n",
    "# Since our results are divided into different files for each day, we have to combine them\n",
    "combined_result = pd.concat([pd.read_csv(filename) for filename in result_filenames])\n",
    "\n",
    "# Save the final result to a CSV file\n",
    "combined_result.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46211c6f-2c65-4aa6-8f5c-16a59189ce4c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To remove the generated temp files\n",
    "for filename in result_filenames:\n",
    "    if os.path.exists(filename):  # Check if the file exists before removing\n",
    "        os.remove(filename)\n",
    "    else:\n",
    "        print(f\"File {filename} not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
