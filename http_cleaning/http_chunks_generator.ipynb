{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c03e7b8-30c7-44c9-8529-8caf74f2487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The input dataset should be the latest r6.2 Cert Insider Threat Dataset\n",
    "\n",
    "# For this dataset, I used HPC Cluster (Magnolia) from University of Southern Mississippi\n",
    "# In HPC clusters, I used Slrum Workload Manager, the script for which is also discussed somewhere in the repo\n",
    "# Because the dataset was huge, I had to resort to splitting dataset into different parts while also maintaing the integrity of the data\n",
    "\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e91d52f7-3739-479d-becc-0c746543555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary folder to store intermediate chunked files\n",
    "temp_folder_chunks= 'temp_chunks'\n",
    "\n",
    "# Dataset\n",
    "dataset_path = 'reduced_http_dataset.csv'\n",
    "# If the folder doesn't exist, create one\n",
    "os.makedirs(temp_folder_chunks, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311dc91b-83bc-4f79-8844-eab0effeee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Group data by day\n",
    "grouped_data = df.groupby('day')\n",
    "\n",
    "# Write grouped chunks to temporary files\n",
    "# Splitting the dataset and saving each files to separately calculaIntroduction to probability, random variables, mathematical expectation, sampling distributions, confidence intervals and hypothesis testing on single populations. te to avoid memory overload\n",
    "# I didnot do parallel processing in this code\n",
    "\n",
    "chunk_filenames = []\n",
    "for name, group in grouped_data:\n",
    "    temp_chunk_filename = f'temp_chunk_{name}.csv'\n",
    "    group.to_csv(f'{temp_folder_chunks}/{temp_chunk_filename}', index=False)\n",
    "    chunk_filenames.append(temp_chunk_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f50e13-5006-4f0c-8895-ccf31133c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the names of the files so that you can access them later\n",
    "with open('chunk_files.log', 'w') as file:\n",
    "    for name in chunk_filenames:\n",
    "        file.write(name + '\\n')\n",
    "\n",
    "# Or you can directly access the file names from the chunks directory\n",
    "# file_names = os.listdir(temp_folder_chunks)\n",
    "# chunk_filenames = [file for file in file_names if os.path.isfile(os.path.join(temp_folder_chunks, file))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
